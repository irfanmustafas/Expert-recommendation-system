{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named graphlab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ecc6a5fde29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named graphlab"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "import evaluate\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qword = pickle.load(open('../features/question_word_wordvec100.p', 'rb'))\n",
    "# uword = pickle.load(open('../features/user_word_wordvec100.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ques = []\n",
    "users = []\n",
    "vals = []\n",
    "with open('../train_data/invited_info_train.txt', 'r') as f1:\n",
    "    for line in f1:\n",
    "        qid, uid, val = line.rstrip('\\n').split()\n",
    "        ques.append(qid)\n",
    "        users.append(uid)\n",
    "        vals.append(int(val))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quesv = []\n",
    "usersv = []\n",
    "#vals = []\n",
    "with open('../train_data/validate_nolabel.txt', 'r') as f1:\n",
    "    line = f1.readline()\n",
    "    for line in f1:\n",
    "        qid, uid =  line.rstrip('\\r\\n').split(',')\n",
    "        quesv.append(qid)\n",
    "        usersv.append(uid)\n",
    "        #vals.append(int(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quest = []\n",
    "userst = []\n",
    "#vals = []\n",
    "with open('../train_data/test_nolabel.txt', 'r') as f1:\n",
    "    line = f1.readline()\n",
    "    for line in f1:\n",
    "        qid, uid =  line.rstrip('\\r\\n').split(',')\n",
    "        quest.append(qid)\n",
    "        userst.append(uid)\n",
    "        #vals.append(int(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf = gl.SFrame({'user_id':users, 'item_id':ques, 'rating':vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfv  = gl.SFrame({'user_id':usersv, 'item_id':quesv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sft  = gl.SFrame({'user_id':userst, 'item_id':quest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "with open('../train_data/user_info.txt', 'r') as f1:\n",
    "    for line in f1:\n",
    "        topic = map(str, (line.split()[1]).split('/'))\n",
    "        topics.append(topic)\n",
    "\n",
    "user_keys = pickle.load(open('../features/user_info_keys.dat', 'rb'))\n",
    "ques_keys = pickle.load(open('../features/question_info_keys.dat', 'rb'))\n",
    "\n",
    "#ufeat = []\n",
    "#for i in range(len(topics)):\n",
    "    #ufeat.append([1 if x in topics[i] else 0 for x in range(145)])\n",
    "questopics = []\n",
    "qnum = []\n",
    "\n",
    "with open('../train_data/question_info.txt', 'r') as f1:\n",
    "    for line in f1:\n",
    "        questopics.append(line.split()[1])\n",
    "        up, tans, ans = map(int, line.split()[4:7])\n",
    "        qnum.append({'upvotes': up, 'topans':tans, 'answers':ans})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_info = gl.SFrame({'user_id': user_keys, 'numeric_feature':uword})\n",
    "ques_info = gl.SFrame({'item_id': ques_keys, 'numeric_feature':qword})\n",
    "print ques_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1 = gl.ranking_factorization_recommender.create(sf, target='rating', ranking_regularization = 0.05, \n",
    "                                                 unobserved_rating_value=-0.5, verbose=False,\n",
    "                                                 solver='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = m1.predict(sfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "#normalization\n",
    "maxscore = max(scores)\n",
    "minscore = min(scores)\n",
    "for score in scores:\n",
    "    predictions.append((score-minscore)/float(maxscore-minscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fname = 'tempfile'\n",
    "fname = '../validation/rankfactor.csv'\n",
    "print len(quesv)\n",
    "with open(fname, 'w') as f1:\n",
    "    f1.write('qid,uid,label\\n')\n",
    "    for i in range(len(quesv)):\n",
    "        f1.write(quesv[i]+','+usersv[i]+','+str(predictions[i])+'\\n')\n",
    "#print evaluate.ndcg(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = m1.predict(sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "#normalization\n",
    "maxscore = max(scores)\n",
    "minscore = min(scores)\n",
    "for score in scores:\n",
    "    predictions.append((score-minscore)/float(maxscore-minscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = '../testsubmissions/rankfactor.csv'\n",
    "print len(quest)\n",
    "with open(fname, 'w') as f1:\n",
    "    f1.write('qid,uid,label\\n')\n",
    "    for i in range(len(quest)):\n",
    "        f1.write(quest[i]+','+userst[i]+','+str(predictions[i])+'\\n')\n",
    "#print evaluate.ndcg(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
